# This file configures Charmcraft.
# See https://juju.is/docs/sdk/charmcraft-config for guidance.

name: opentelemetry-collector-k8s
type: charm
summary: Vendor-agnostic way to receive, process and export telemetry data.
description: |
  The OpenTelemetry Collector offers a vendor-agnostic implementation on how to receive,
  process and export telemetry data. In addition, it removes the need to run, operate and
  maintain multiple agents/collectors in order to support open-source telemetry data formats
  (e.g. Jaeger, Prometheus, etc.) to multiple open-source or commercial back-ends.

links:
  # documentation: https://discourse.charmhub.io/
  website: https://charmhub.io/opentelemetry-collector-k8s
  source: https://github.com/canonical/opentelemetry-collector-k8s-operator
  issues: https://github.com/canonical/opentelemetry-collector-k8s-operator/issues

assumes:
  - k8s-api
  - juju >= 3.6

platforms:
  ubuntu@24.04:amd64:
  ubuntu@24.04:arm64:

parts:
  charm:
    source: .
    plugin: uv
    build-packages: [git]
    build-snaps: [astral-uv]
    override-build: |
      craftctl default
      git describe --always > $CRAFT_PART_INSTALL/version
  cos-tool:
    plugin: dump
    source: https://github.com/canonical/cos-tool/releases/latest/download/cos-tool-${CRAFT_ARCH_BUILD_FOR}
    source-type: file
    permissions:
      - path: cos-tool-${CRAFT_ARCH_BUILD_FOR}
        mode: "755"

containers:
  otelcol:
    resource: opentelemetry-collector-image

resources:
  opentelemetry-collector-image:
    type: oci-image
    description: OCI image for opentelemetry-collector
    upstream-source: ubuntu/opentelemetry-collector:0.127-24.04

provides:
  grafana-dashboards-provider:
    interface: grafana_dashboard
    description: |
      Send own and aggregated dashboards to grafana.
  receive-loki-logs:
    interface: loki_push_api
    optional: true
    description: To receive logs by allowing Promtail instances to specify otelcol as their Loki address.
  receive-traces:
    interface: tracing
    optional: true
    description: Receive traces from other charms.
  # TODO https://github.com/open-telemetry/opentelemetry-collector-contrib/issues/37277
  # receive-remote-write:
  #   interface: prometheus_remote_write

requires:
  grafana-dashboards-consumer:
    interface: grafana_dashboard
    description: |
      Collect dashboards to be forwarded to grafana.
  metrics-endpoint:
    interface: prometheus_scrape
    optional: true
    description: To scrape other charms' metrics endpoints.
  send-remote-write:
    interface: prometheus_remote_write
    optional: true
    description: To forward collected metrics to a Prometheus backend.
  send-loki-logs:
    interface: loki_push_api
    optional: true
    description: To forward collected logs to a Loki backend.
  receive-ca-cert:
    interface: certificate_transfer
    optional: true
    description: |
      For otelcol to create a trusted (TLS) connection with servers
      it scrapes, pushes to, etc., it needs to trust the CA that signed
      the server it talks to. CA certs forwarded over this relation are
      installed in the root CA store.
  cloud-config:
    interface: grafana_cloud_config
    optional: true
    limit: 1
    description: Forward telemetry to another Observability stack (Grafana Cloud, COS, etc.).
  receive-server-cert:
    interface: tls-certificates
    optional: true
    limit: 1
    description: |
      Obtain a certificate from a CA.
      The charm sends a CSR over this relation and receives a signed certificate back.
      That is the cert that would be presented to incoming TLS connections.
  # TODO: remove limit once https://github.com/canonical/tempo-coordinator-k8s-operator/issues/168 is resolved
  send-traces:
    interface: tracing
    limit: 1
    optional: true
    description: Send traces to a charmed Tempo instance.

config:
  options:
    processors:
      type: string
      description: |
        A global opentelemetry-collector "processors" config in YAML format, without the
        "processors:" top-level key. For example, to represent a "processors" section
         such as:

            	processors:
               batch:
               memory_limiter:
                 limit_mib: 4000

        you could use `juju config otelcol processors=@processors.yaml`, with:

            	# processors.yaml
            	batch:
            	memory_limiter:
               limit_mib: 4000

        The provided processors section will be applied to all relevant pipelines.
        
        Reference: https://opentelemetry.io/docs/collector/configuration/#processors
    forward_alert_rules:
      description: >
        Toggle forwarding of alert rules.
      type: boolean
      default: true
    tls_insecure_skip_verify:
      description: |
        Flag to skip the validation of certificates from servers we connect to with TLS.
        If "true", self-signed certs can be used seamlessly; this setting will be applied
        to all the otelcol exporter configurations and any receivers which actively make
        requests to servers, e.g. the prometheus receiver scraping metrics endpoints.
      type: boolean
      default: false
    # Tracing config options
    always_enable_zipkin:
      description: >
        Force-enable the receiver for the 'zipkin' protocol in OpenTelemetry Collector, 
        even if there is no integration currently requesting it.
      type: boolean
      default: false
    always_enable_jaeger_grpc:
      description: >
        Force-enable the receiver for the 'jaeger_grpc' protocol in OpenTelemetry Collector, 
        even if there is no integration currently requesting it.
      type: boolean
      default: false
    always_enable_jaeger_thrift_http:
      description: >
        Force-enable the receiver for the 'jaeger_thrift_http' protocol in OpenTelemetry Collector, 
        even if there is no integration currently requesting it.
      type: boolean
      default: false
    tracing_sampling_rate_charm:
      description: >
        This property defines the percentage of charm traces that are sent to the tracing backend.
        Setting it to 100 would mean all charm traces are kept, setting to 0 means charm traces
        aren't sent to the tracing backend at all. Anything outside of 0-100 range will be clamped 
        to this range by OpenTelemetry Collector.
      type: float
      default: 100.0
    tracing_sampling_rate_workload:
      description: >
        This property defines the percentage of workload traces that are sent to the tracing backend.
        Setting it to 100 would mean all workload traces are kept, setting to 0 means workload traces
        aren't sent to the tracing backend at all. Anything outside of 0-100 range will be clamped 
        to this range by OpenTelemetry Collector.
      type: float
      default: 1.0
    tracing_sampling_rate_error:
      description: >
        This property defines the percentage of error traces (from all sources) that are sent to the tracing backend.
        Setting it to 100 would mean all error traces are kept, setting to 0 means error traces
        aren't sent to the tracing backend at all. Anything outside of 0-100 range will be clamped 
        to this range by OpenTelemetry Collector.
      type: float
      default: 100.0

actions:
  reconcile:
    description: >
      Regenerate the world state from scratch. This includes the configuration file,
      but also relation data set by the charm.
